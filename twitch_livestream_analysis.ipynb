{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bac56bb",
   "metadata": {},
   "source": [
    "## Analysis of Live Streaming Health Metrics (Using Twitch Interaction Data)\n",
    "\n",
    "### Project Introduction\n",
    "\n",
    "This report conducts an Exploratory Data Analysis (EDA) on a Twitch live-streaming interaction dataset. The primary goal is to understand user and streamer behavior to inform the design of a \"health metric\" framework for live-streaming platforms.\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "1. To process and analyze a large dataset using a combination of Pandas and PySpark.\n",
    "\n",
    "2. To calculate key metrics related to platform activity, user engagement, and content supply.\n",
    "\n",
    "3. To visualize trends and distributions to uncover initial insights.\n",
    "\n",
    "### Dataset:\n",
    "The dataset contains a sample of 100,000 users' viewing interactions on Twitch. The columns are:\n",
    "\n",
    "- UserId: Unique identifier for a viewer.\n",
    "\n",
    "- StreamID: Unique identifier for a specific live stream segment.\n",
    "\n",
    "- Streamer: The name of the streamer.\n",
    "\n",
    "- StartTime: The time (in minutes from a relative start) the user began watching.\n",
    "\n",
    "- StopTime: The time (in minutes) the user stopped watching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c95c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"100k_a.csv\"\n",
    "column_names = ['UserId', 'StreamID', 'Streamer', 'StartTime', 'StopTime']\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"volodymyrpivoshenko/twitch-live-streaming-interactions-sample-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    "  pandas_kwargs={\"header\": None, \"names\": column_names}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016a3a8",
   "metadata": {},
   "source": [
    "### 2. Data Preparation and PySpark Initialization\n",
    "\n",
    "The data is initially loaded into a Pandas DataFrame for convenience. To handle potentially larger datasets and leverage distributed computing capabilities, we will convert it into a PySpark DataFrame. This section initializes the Spark session and prepares the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d55181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data check\n",
    "df.shape\n",
    "df.head()\n",
    "!java -version\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Twitch Live Health Analysis\").getOrCreate()\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca714641",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing and Quality Assessment\n",
    "\n",
    "Before diving into analysis, it's crucial to assess the quality of the data. We will check for correct data types (schema) and look for any missing or null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "def data_type_diagnostics(df):\n",
    "    print(\"===== Data type（Schema）=====\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "data_type_diagnostics(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_diagnostics(df):\n",
    "    exprs = []\n",
    "    for c, t in df.dtypes:\n",
    "        if t in ('bigint', 'int', 'double', 'float'):\n",
    "            exprs.append(count(when(col(c).isNull() | isnan(col(c)), c)).alias(c))\n",
    "        elif t == 'string':\n",
    "            exprs.append(count(when(col(c).isNull() | (col(c) == \"\"), c)).alias(c))\n",
    "    df.select(exprs).show()\n",
    "\n",
    "data_quality_diagnostics(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745126f",
   "metadata": {},
   "source": [
    "#### Finding:\n",
    "The data quality check reveals that the dataset is remarkably clean. All columns have appropriate data types, and there are zero null or empty values. This allows us to proceed directly to the analysis phase without needing extensive data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a1a7c",
   "metadata": {},
   "source": [
    "### 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we will compute various metrics to understand the platform's overall scale, user engagement patterns, and content dynamics.\n",
    "\n",
    "### 4.1. High-Level Platform Metrics\n",
    "\n",
    "We begin by calculating fundamental metrics to understand the size and scope of the platform based on this dataset. This includes the number of unique users, streamers, and stream segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, countDistinct, explode, max as spark_max, sequence, col\n",
    "\n",
    "# Basic statistics\n",
    "def basic_statistics(df):\n",
    "    # Number of users\n",
    "    user_count = df.select(\"UserId\").distinct().count()\n",
    "    print(f\"Unique User Count: {user_count}\")\n",
    "\n",
    "    # Number of streamers\n",
    "    streamer_count = df.select(\"Streamer\").distinct().count()\n",
    "    print(f\"Unique Streamer Count: {streamer_count}\")\n",
    "\n",
    "    # Total number of stream segments\n",
    "    total_streams = df.select(\"StreamID\").distinct().count()\n",
    "    print(f\"Total StreamID Count: {total_streams}\")\n",
    "\n",
    "    # Average number of segments per streamer\n",
    "    avg_streams_per_streamer = df.groupBy(\"Streamer\").count().agg(avg(\"count\"))\n",
    "    avg_streams_per_streamer.show()\n",
    "\n",
    "    # Number of viewers per StreamID\n",
    "    watchers_per_stream = df.groupBy(\"StreamID\").agg(countDistinct(\"UserId\").alias(\"WatcherCount\"))\n",
    "    watchers_per_stream.select(avg(\"WatcherCount\").alias(\"AvgWatcherCount\")).show()\n",
    "\n",
    "    # Streamers with the highest number of concurrent viewers in their streams\n",
    "    # 1. Expand each user's time slots in the stream segment\n",
    "    df_expanded = df.withColumn(\n",
    "        \"TimeSlot\",\n",
    "        explode(sequence(col(\"StartTime\"), col(\"StopTime\") - 1))\n",
    "    )\n",
    "\n",
    "    # 2. Aggregate user count by StreamID + TimeSlot\n",
    "    users_per_slot = df_expanded.groupBy(\"StreamID\", \"TimeSlot\") \\\n",
    "        .agg(countDistinct(\"UserId\").alias(\"UsersCount\"))\n",
    "\n",
    "    # 3. Get the maximum user count for each StreamID\n",
    "    max_concurrent_users = users_per_slot.groupBy(\"StreamID\") \\\n",
    "        .agg(spark_max(\"UsersCount\").alias(\"MaxConcurrentUsers\"))\n",
    "\n",
    "    # 4. Show results\n",
    "    max_concurrent_users.orderBy(\"MaxConcurrentUsers\", ascending=False).show(10)\n",
    "\n",
    "    # Match StreamID to Streamer, take the maximum and deduplicate by streamer\n",
    "    max_concurrent_users_with_streamer = max_concurrent_users.join(\n",
    "        df.select(\"StreamID\", \"Streamer\").distinct(),\n",
    "        on=\"StreamID\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    max_concurrent_users_with_streamer = max_concurrent_users_with_streamer.dropDuplicates([\"Streamer\"]).orderBy(\"MaxConcurrentUsers\", ascending=False)\n",
    "    max_concurrent_users_with_streamer.show(10)\n",
    "\n",
    "basic_statistics(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636036e",
   "metadata": {},
   "source": [
    "#### Summary of High-Level Metrics:\n",
    "\n",
    "- Unique Users: 100,000\n",
    "\n",
    "- Unique Streamers: 162,625. This is a fascinating insight: in this sample, there are more streamers than unique viewers, suggesting a very large supply of content creators relative to the viewer base.\n",
    "\n",
    "- Total Stream Segments (StreamIDs): 739,991\n",
    "\n",
    "- Average Streams per Streamer: ~18.8\n",
    "\n",
    "- Average Watchers per Stream: ~4.1. This low number suggests that the vast majority of streams have a very small audience, pointing towards a long-tail distribution of popularity.\n",
    "\n",
    "- Max Concurrent Users: The analysis shows that top streamers like pokemon, cloud9, and nintendo can attract a significant number of concurrent viewers (over 150-200), standing in sharp contrast to the average.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8577750",
   "metadata": {},
   "source": [
    "### 4.2. User Behavior Metrics (Watch Duration & Time Analysis)\n",
    "\n",
    "Next, we analyze how users engage with content, focusing on the duration of their viewing sessions. We also convert the relative timestamps into actual datetime objects to enable time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing duration distribution\n",
    "from pyspark.sql.functions import (avg, max as spark_max, min as spark_min)\n",
    "\n",
    "df_with_duration = spark_df.withColumn(\"WatchDuration\", col(\"StopTime\") - col(\"StartTime\"))\n",
    "df_with_duration.select(\n",
    "    avg(\"WatchDuration\").alias(\"AvgWatchDuration\"),\n",
    "    spark_min(\"WatchDuration\").alias(\"MinWatchDuration\"),\n",
    "    spark_max(\"WatchDuration\").alias(\"MaxWatchDuration\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd8bc9",
   "metadata": {},
   "source": [
    "#### Finding:\n",
    "The average watch duration is only 3.14 minutes (since StartTime and StopTime are in minutes). This suggests that many interactions are very short, possibly representing users \"channel surfing\" or quickly checking out different streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no given start date, assume it begins on August 1, 2025\n",
    "# Start date conversion\n",
    "from pyspark.sql.functions import expr\n",
    "base_date = \"2025-08-01 00:00:00\"\n",
    "\n",
    "df_with_datetime = spark_df.withColumn(\n",
    "    \"StartDateTime\",\n",
    "    expr(f\"timestamp('{base_date}') + INTERVAL 1 MINUTES * StartTime\")\n",
    ").withColumn(\n",
    "    \"StopDateTime\",\n",
    "    expr(f\"timestamp('{base_date}') + INTERVAL 1 MINUTES * StartTime\")\n",
    ")\n",
    "\n",
    "# or expr(f\"timestamp('{base_date}') + make_interval(0, 0, 0, 0, 0, StartTime*10, 0)\")\n",
    "\n",
    "df_with_datetime.select(\"UserId\", \"Streamer\", \"StartTime\", \"StartDateTime\", \"StopDateTime\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8d0b3",
   "metadata": {},
   "source": [
    "### Data Assumption:\n",
    "Since the dataset provides relative time in minutes, a base date of 2025-08-01 has been assumed to facilitate time-series analysis. This is an artificial construct necessary for this analysis, and any conclusions about specific dates should be viewed with this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily statistics of active users, number of broadcasts, and platform peak concurrent users\n",
    "from pyspark.sql.functions import to_date\n",
    "df_with_day = df_with_datetime.withColumn(\n",
    "    \"Day\",\n",
    "    to_date(col(\"StartDateTime\"))\n",
    ")\n",
    "df_with_day.show(10)\n",
    "\n",
    "daily_active_users_count = df_with_day.groupBy(\"Day\").agg(countDistinct(\"UserId\").alias(\"ActiveUserCount\"))\n",
    "\n",
    "daily_active_streamer_count = df_with_day.groupBy(\"Day\").agg(countDistinct(\"Streamer\").alias(\"ActiveStreamerCount\"))\n",
    "\n",
    "df_with_day_expanded = df_with_day.withColumn(\"TimeSlot\", explode(sequence(col(\"StartTime\"), col(\"StopTime\") - 1)))\n",
    "df_with_day_expanded = df_with_day_expanded.withColumn(\"SlotDatetime\", expr(f\"timestamp('{base_date}') + INTERVAL 1 MINUTES * TimeSlot\"))\n",
    "users_per_slot = df_with_day_expanded.groupBy(\"Day\", \"SlotDatetime\").agg(countDistinct(\"UserId\").alias(\"UsersCountPerSlot\"))\n",
    "daily_max_concurrent_users = users_per_slot.groupBy(\"Day\").agg(spark_max(\"UsersCountPerSlot\").alias(\"MaxConcurrentUsers\"))\n",
    "\n",
    "daily_summary_per_slot = daily_active_users_count.join(daily_active_streamer_count, on='Day', how='left').join(users_per_slot, on='Day', how='left').orderBy(\"UsersCountPerSlot\")\n",
    "daily_summary_per_day = daily_active_streamer_count.join(daily_active_users_count, on='Day', how='left').join(daily_max_concurrent_users, on='Day', how='left') \n",
    "\n",
    "daily_summary_per_slot.show(10)\n",
    "daily_summary_per_day.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be484d58",
   "metadata": {},
   "source": [
    "### 4.3. Visual Analysis\n",
    "\n",
    "Visualizations are essential for understanding trends and distributions. We will now plot the metrics calculated above.\n",
    "\n",
    "#### Daily Activity Trends\n",
    "\n",
    "This plot shows the daily active users, active streamers, and the platform's peak concurrent users over the analysis period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "daily_summary_per_day_pd = daily_summary_per_day.toPandas()\n",
    "daily_summary_per_day_pd = daily_summary_per_day_pd.sort_values(\"Day\") \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "\n",
    "ax1.plot(daily_summary_per_day_pd['Day'], daily_summary_per_day_pd['ActiveUserCount'], marker='o', label='Active Users', color='blue')\n",
    "ax1.plot(daily_summary_per_day_pd['Day'], daily_summary_per_day_pd['ActiveStreamerCount'], marker='o', label='Active Streamers', color='green')\n",
    "\n",
    "ax2.plot(daily_summary_per_day_pd['Day'], daily_summary_per_day_pd['MaxConcurrentUsers'], marker='o', label='Max Concurrent Users', color='red')\n",
    "\n",
    "ax1.set_xlabel(\"Day\")\n",
    "ax1.set_ylabel(\"Users/Streamers\")\n",
    "ax2.set_ylabel(\"Max Concurrent Users\")\n",
    "\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.title(\"Daily Twitch Activity Metrics\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c0b38",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "The plot reveals daily fluctuations in platform activity. The number of active users and streamers generally move in tandem. The peak concurrent users (red line) show a slightly different pattern, indicating that overall activity doesn't always translate to higher peak concentration. A longer time series would be needed to confirm if a weekly cyclical pattern (e.g., weekend peaks) exists.\n",
    "\n",
    "---\n",
    "\n",
    "#### Distribution of Watch Durations\n",
    "This histogram shows the frequency of different watch durations. A logarithmic scale is used on the y-axis to better visualize the long-tail nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d480b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_duration = spark_df.withColumn(\"WatchDuration\", col(\"StopTime\") - col(\"StartTime\"))\n",
    "duration_pd = df_with_duration.select(\"WatchDuration\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(duration_pd[\"WatchDuration\"], bins=50, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Watch Duration (10-min units)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Watch Durations\")\n",
    "plt.yscale(\"log\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004131c",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "The histogram is heavily skewed to the right, confirming the low average watch duration found earlier. The logarithmic scale highlights a classic long-tail distribution: a massive number of very short interactions (1-2 minutes) and a rapidly decreasing number of longer viewing sessions. This is typical user behavior on content discovery platforms.\n",
    "\n",
    "---\n",
    "\n",
    "#### Distribution of Watchers per Stream\n",
    "This histogram shows how many streams have a certain number of unique viewers. Again, a log scale is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f976b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchers_per_stream = spark_df.groupBy(\"StreamID\").agg(countDistinct(\"UserId\").alias(\"WatcherCount\"))\n",
    "watchers_pd = watchers_per_stream.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(watchers_pd[\"WatcherCount\"], bins=50, color=\"orange\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Unique Watchers per Stream\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Watchers per StreamID\")\n",
    "plt.yscale(\"log\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cdb835",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "Similar to watch duration, the number of unique watchers per stream also follows a long-tail pattern. The vast majority of streams have only one or two viewers. This reinforces the \"average watchers\" metric and shows that only a small fraction of streams become popular, while most content has a very niche audience.\n",
    "\n",
    "#### Streamer Activity vs. Popularity\n",
    "This scatter plot explores the relationship between how frequently a streamer broadcasts (Stream Count) and how many unique viewers they attract across all their streams. Both axes are logarithmic to better handle the wide range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_stats = spark_df.groupBy(\"Streamer\") \\\n",
    "    .agg(countDistinct(\"StreamID\").alias(\"StreamCount\"),\n",
    "         countDistinct(\"UserId\").alias(\"UniqueWatchers\"))\n",
    "\n",
    "streamer_pd = streamer_stats.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(streamer_pd[\"StreamCount\"], streamer_pd[\"UniqueWatchers\"], alpha=0.5)\n",
    "plt.xlabel(\"Number of Streams per Streamer\")\n",
    "plt.ylabel(\"Unique Watchers\")\n",
    "plt.title(\"Streamer Stream Count vs Unique Watchers\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef15e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activity = spark_df.groupBy(\"UserId\").count().withColumnRenamed(\"count\", \"InteractionCount\")\n",
    "user_pd = user_activity.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(user_pd[\"InteractionCount\"], bins=50, color=\"purple\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Interactions per User\")\n",
    "plt.ylabel(\"Number of Users\")\n",
    "plt.title(\"User Activity Distribution\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974f6e7",
   "metadata": {},
   "source": [
    "#### Insight:\n",
    "The plot shows a clear positive correlation: streamers who broadcast more tend to attract a larger unique audience. However, the relationship is not strictly linear. For any given stream count, there is a wide variance in the number of unique watchers. This suggests that while consistency (higher stream count) is important, it is not the only factor for success. Content quality, streamer personality, or existing fame likely play a significant role, allowing some streamers to achieve high viewership with relatively few streams.\n",
    "\n",
    "### 5. Conclusion and Next Steps\n",
    "\n",
    "This initial EDA has provided several key insights into the platform's dynamics:\n",
    "\n",
    "- High Content Supply: The number of streamers is very high relative to the sampled user base.\n",
    "\n",
    "- Long-Tail Engagement: User engagement, both in terms of watch duration and stream popularity, follows a strong long-tail distribution. Most interactions are short, and most streams have few viewers.\n",
    "\n",
    "- Activity Correlates with Popularity: There is a positive, though not deterministic, relationship between the number of times a streamer goes live and the size of their audience.\n",
    "\n",
    "### Next Steps for a Deeper Analysis:\n",
    "\n",
    "- User/Streamer Segmentation: Classify users (e.g., casual vs. power users) and streamers (e.g., top-tier, mid-tier, long-tail) to analyze their behaviors separately.\n",
    "\n",
    "- Retention Analysis: Although difficult without explicit dates, a cohort analysis could be performed on the relative timeline to understand user retention, a key health metric.\n",
    "\n",
    "- Time-of-Day Analysis: With a corrected StopDateTime, analyze peak viewing hours to understand platform usage patterns better.\n",
    "\n",
    "- Refine Health Metrics: Based on these findings, a more formal \"health framework\" could be proposed, incorporating metrics for Content Richness (streamer count), User Engagement (DAU, watch duration distribution), and Content Discovery (how well the platform surfaces long-tail content)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
